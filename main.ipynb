{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NanoPy Training Pipeline\n",
    "\n",
    "This notebook trains a GPT-style model on Python code. It has been optimized for performance and observability.\n",
    "\n",
    "**Improvements:**\n",
    "- **Flash Attention**: Uses PyTorch 2.0 scaled dot product attention.\n",
    "- **Efficient Data Loading**: Uses raw tokenization and `map` for memory efficiency.\n",
    "- **Monitoring**: Live loss plots and text generation.\n",
    "- **Checkpointing**: Saves the best model and periodic checkpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "import tiktoken\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Import model and config from local file\n",
    "from model import NanoPy, GPTConfig\n",
    "\n",
    "# Configuration\n",
    "BATCH_SIZE = 8       # Micro-batch size\n",
    "ACCUM_STEPS = 4      # Gradient accumulation steps\n",
    "TOTAL_BATCH_SIZE = BATCH_SIZE * ACCUM_STEPS\n",
    "LEARNING_RATE = 3e-4\n",
    "MAX_STEPS = 5000     # Total optimization steps\n",
    "WARMUP_STEPS = 100\n",
    "VALIDATE_EVERY = 250\n",
    "GENERATE_EVERY = 500\n",
    "SAVE_EVERY = 1000\n",
    "BLOCK_SIZE = 512\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "OUTPUT_DIR = \"checkpoints\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Total Batch Size: {TOTAL_BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Preparation (Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "ds = load_dataset(\"jtatman/python-code-dataset-500k\", split=\"train\")\n",
    "\n",
    "# Tokenizer\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "eos_token = enc.eot_token\n",
    "\n",
    "def process_and_tokenize(examples):\n",
    "    # Batch processing for speed\n",
    "    instructions = examples['instruction']\n",
    "    outputs = examples['output']\n",
    "    \n",
    "    batch_input_ids = []\n",
    "    batch_labels = []\n",
    "    \n",
    "    for inst, out in zip(instructions, outputs):\n",
    "        # Format text\n",
    "        text = f'\"\"\"\\n{inst}\\n\"\"\"\\n{out}'\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = enc.encode(text)\n",
    "        tokens.append(eos_token)\n",
    "        \n",
    "        # Truncate/Pad\n",
    "        if len(tokens) > BLOCK_SIZE + 1:\n",
    "            tokens = tokens[:BLOCK_SIZE + 1]\n",
    "        else:\n",
    "            tokens = tokens + [eos_token] * (BLOCK_SIZE + 1 - len(tokens))\n",
    "            \n",
    "        # Create inputs (0:-1) and targets (1:)\n",
    "        batch_input_ids.append(tokens[:-1])\n",
    "        batch_labels.append(tokens[1:])\n",
    "        \n",
    "    return {\"input_ids\": batch_input_ids, \"labels\": batch_labels}\n",
    "\n",
    "print(\"Processing dataset... This might take a moment but is memory efficient.\")\n",
    "# Use batched map for speed\n",
    "tokenized_ds = ds.map(\n",
    "    process_and_tokenize, \n",
    "    batched=True, \n",
    "    remove_columns=ds.column_names, \n",
    "    num_proc=4\n",
    ")\n",
    "tokenized_ds.set_format(type='torch', columns=['input_ids', 'labels'])\n",
    "\n",
    "# Split Train/Val\n",
    "split_ds = tokenized_ds.train_test_split(test_size=0.01, seed=42)\n",
    "train_ds = split_ds['train']\n",
    "val_ds = split_ds['test']\n",
    "\n",
    "print(f\"Train size: {len(train_ds)}\")\n",
    "print(f\"Val size: {len(val_ds)}\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Model\n",
    "config = GPTConfig(\n",
    "    vocab_size=50257,\n",
    "    block_size=BLOCK_SIZE,\n",
    "    n_layer=12,\n",
    "    n_head=8,\n",
    "    n_embd=768,\n",
    "    dropout=0.1,\n",
    "    bias=True\n",
    ")\n",
    "model = NanoPy(config).to(DEVICE)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-1)\n",
    "scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
    "\n",
    "print(f\"Model parameters: {model.get_num_params()/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(step):\n",
    "    if step < WARMUP_STEPS:\n",
    "        return LEARNING_RATE * step / WARMUP_STEPS\n",
    "    if step > MAX_STEPS:\n",
    "        return LEARNING_RATE * 0.1\n",
    "    decay_ratio = (step - WARMUP_STEPS) / (MAX_STEPS - WARMUP_STEPS)\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return LEARNING_RATE * coeff\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, loader, eval_iters=50):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for i, batch in enumerate(loader):\n",
    "        if i >= eval_iters: break\n",
    "        X, Y = batch['input_ids'].to(DEVICE), batch['labels'].to(DEVICE)\n",
    "        with torch.amp.autocast(device_type=DEVICE, dtype=torch.float16) if scaler else torch.no_grad():\n",
    "            _, loss = model(X, Y)\n",
    "        losses.append(loss.item())\n",
    "    model.train()\n",
    "    return sum(losses) / len(losses)\n",
    "\n",
    "def generate_sample(model, prompt=\"def fibonacci(\"):\n",
    "    model.eval()\n",
    "    idx = enc.encode(prompt)\n",
    "    idx = torch.tensor(idx, dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(idx, max_new_tokens=100, temperature=0.8, top_k=50)\n",
    "        \n",
    "    model.train()\n",
    "    return enc.decode(generated[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop with Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "step = 0\n",
    "t0 = time.time()\n",
    "\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# Ensure loader is iterable once\n",
    "iter_loader = iter(train_loader)\n",
    "\n",
    "while step < MAX_STEPS:\n",
    "    \n",
    "    # Optimization step\n",
    "    optimizer.zero_grad()\n",
    "    accum_loss = 0.0\n",
    "    \n",
    "    for _ in range(ACCUM_STEPS):\n",
    "        try:\n",
    "            batch = next(iter_loader)\n",
    "        except StopIteration:\n",
    "            iter_loader = iter(train_loader)\n",
    "            batch = next(iter_loader)\n",
    "            \n",
    "        X, Y = batch['input_ids'].to(DEVICE), batch['labels'].to(DEVICE)\n",
    "        \n",
    "        with torch.amp.autocast(device_type=DEVICE, dtype=torch.float16) if scaler else torch.no_grad():\n",
    "            _, loss = model(X, Y)\n",
    "            loss = loss / ACCUM_STEPS\n",
    "        \n",
    "        accum_loss += loss.item()\n",
    "        if scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "    # Gradient Clipping & Optimizer Step\n",
    "    if scaler:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    else:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    # Adjust LR\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    step += 1\n",
    "    train_losses.append(accum_loss)\n",
    "\n",
    "    # --- Monitoring ---\n",
    "    if step % 10 == 0:\n",
    "        # Print inline progress\n",
    "        print(f\"Step {step} | Loss: {accum_loss:.4f} | LR: {lr:.2e} | Time: {time.time()-t0:.2f}s\", end='\\r')\n",
    "        t0 = time.time()\n",
    "\n",
    "    if step % VALIDATE_EVERY == 0:\n",
    "        val_loss = estimate_loss(model, val_loader)\n",
    "        val_losses.append((step, val_loss))\n",
    "        print(f\"\\nStep {step} Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Save Best Model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, \"best_model.pt\"))\n",
    "            print(\"Saved new best model!\")\n",
    "            \n",
    "        # Plotting\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(train_losses, label='Train Loss', alpha=0.3)\n",
    "        val_x, val_y = zip(*val_losses)\n",
    "        plt.plot(val_x, val_y, label='Val Loss', marker='o', color='red')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    if step % GENERATE_EVERY == 0:\n",
    "        print(\"\\n--- Sample Generation ---\")\n",
    "        print(generate_sample(model))\n",
    "        print(\"-------------------------\\n\")\n",
    "        \n",
    "    if step % SAVE_EVERY == 0:\n",
    "         torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, f\"ckpt_step_{step}.pt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TanCode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
